{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np #numpy\n",
    "import pandas as pd #pandas\n",
    "import requests #requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup #BeautifulSoup\n",
    "import datetime #datetime\n",
    "import re #regularexpressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the .csv file stored locally\n",
    "nsidc_df_rawscrape = pd.read_csv('2015nsidcAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>unique_users_ip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g02186</td>\n",
       "      <td>24447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g02135</td>\n",
       "      <td>22119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g00472</td>\n",
       "      <td>5862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nsidc-0081</td>\n",
       "      <td>5629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nsidc-0051</td>\n",
       "      <td>4526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           dataset_id  unique_users_ip\n",
       "0  g02186                                                        24447\n",
       "1  g02135                                                        22119\n",
       "2  g00472                                                         5862\n",
       "3  nsidc-0081                                                     5629\n",
       "4  nsidc-0051                                                     4526"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set new column width max\n",
    "pd.set_option('max_colwidth',150)\n",
    "# Check dataframe\n",
    "nsidc_df_rawscrape.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of NSIDC Dataset IDs to scrape course data from (used in url)\n",
    "nsidc_df_rawscrape['dataset_id'] = nsidc_df_rawscrape['dataset_id'].astype('str')\n",
    "datasetid = nsidc_df_rawscrape['dataset_id'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Date time code adapted from: http://www.saltycrane.com/blog/2008/06/how-to-get-current-date-and-time-in/\n",
    "\n",
    "# BeautifulSoup code and requests code adapted from https://www.youtube.com/watch?v=f2h41uEi0xU \n",
    "# and https://www.youtube.com/watch?v=3xQTJi2tqgk\n",
    "\n",
    "# Create empty lists to append data\n",
    "spatial_coverage = []\n",
    "data_format = []\n",
    "scrape_date = []\n",
    "scrape_time = []\n",
    "spatial_resolution = []\n",
    "temporal_coverage = []\n",
    "temporal_resolution = []\n",
    "parameters = []\n",
    "platforms = []\n",
    "sensors = []\n",
    "version = []\n",
    "contributors = []\n",
    "\n",
    "# set i = 0 for the while loop\n",
    "i = 0\n",
    "# Loop through the datasets using the dataset ids\n",
    "while i < len(datasetid):\n",
    "    # The url is dependent upon the specific datasetid\n",
    "    url = \"https://nsidc.org/data/\" +datasetid[i]\n",
    "    # Pull the webpage using requests\n",
    "    r = requests.get(url)\n",
    "    # Create a BeautifulSoup object to hold the content of the url\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    # Create an object to hold the data found to match the html td class for spatial coverage\n",
    "    x_1 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-dataset-geo-coordinates views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the spatial_coverage list, if no contents add empty string\n",
    "    if len(x_1) == 0:\n",
    "        spatial_coverage.append(\"\")\n",
    "    else:\n",
    "        for item in x_1:\n",
    "            spatial_coverage.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for data format\n",
    "    x_2 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-dataset-format views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the data_format list, if no contents add empty string\n",
    "    if len(x_2) == 0:\n",
    "        data_format.append(\"\")\n",
    "    else:\n",
    "        for item in x_2:\n",
    "            data_format.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for spatial resolution\n",
    "    x_3 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-spatial-resolution-lat-lon views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the spatial resolution list, if no contents add empty string\n",
    "    if len(x_3) == 0:\n",
    "        spatial_resolution.append(\"\")\n",
    "    else:\n",
    "        for item in x_3:\n",
    "            spatial_resolution.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for temporal coverage\n",
    "    x_4 = soup.find_all(\"td\", {\"class\": \"views-field views-field-views-conditional-1 views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the temporal coverage list, if no contents add empty string\n",
    "    if len(x_4) == 0:\n",
    "        temporal_coverage.append(\"\")\n",
    "    else:\n",
    "        for item in x_4:\n",
    "            temporal_coverage.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for temporal resolution\n",
    "    x_5 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-dataset-temporal-resolutio views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the temporal resolution list, if no contents add empty string\n",
    "    if len(x_5) == 0:\n",
    "        temporal_resolution.append(\"\")\n",
    "    else:\n",
    "        for item in x_5:\n",
    "            temporal_resolution.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for parameters\n",
    "    x_6 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-dataset-parameter-gcmd views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the parameters list, if no contents add empty string\n",
    "    if len(x_6) == 0:\n",
    "        parameters.append(\"\")\n",
    "    else:\n",
    "        for item in x_6:\n",
    "            parameters.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for platforms\n",
    "    x_7 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-dataset-platform views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the platforms list, if no contents add empty string\n",
    "    if len(x_7) == 0:\n",
    "        platforms.append(\"\")\n",
    "    else:\n",
    "        for item in x_7:\n",
    "            platforms.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for sensors\n",
    "    x_8 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-dataset-sensor data-sensors views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the sensors list, if no contents add empty string\n",
    "    if len(x_8) == 0:\n",
    "        sensors.append(\"\")\n",
    "    else:\n",
    "        for item in x_8:\n",
    "            sensors.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for version\n",
    "    x_9 = soup.find_all(\"td\", {\"class\": \"views-field views-field-field-dataset-version views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the version list, if no contents add empty string\n",
    "    if len(x_9) == 0:\n",
    "        version.append(\"\")\n",
    "    else:\n",
    "        for item in x_9:\n",
    "            version.append(item.text)\n",
    "    # Create an object to hold the data found to match the html td class for contributors\n",
    "    x_10 = soup.find_all(\"td\", {\"class\": \"views-field views-field-views-conditional data-contributors views-column-odd views-column-first views-column-last\"})\n",
    "    # Loop through each item and add the contents to the contributors list, if no contents add empty string\n",
    "    if len(x_10) == 0:\n",
    "        contributors.append(\"\")\n",
    "    else:\n",
    "        for item in x_10:\n",
    "            contributors.append(item.text)\n",
    "    # Store the current date in scrape_date\n",
    "    scrape_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    # Store the current time in scrape_time\n",
    "    scrape_time = datetime.datetime.now().strftime(\"%H:%M:%S%p\")\n",
    "    # Increase i by 1 and run through loop\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Citation Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Date time code adapted from: http://www.saltycrane.com/blog/2008/06/how-to-get-current-date-and-time-in/\n",
    "\n",
    "# BeautifulSoup code and requests code adapted from https://www.youtube.com/watch?v=f2h41uEi0xU \n",
    "# and https://www.youtube.com/watch?v=3xQTJi2tqgk\n",
    "\n",
    "# Create empty lists to append data\n",
    "DOI = []\n",
    "citation_date = []\n",
    "\n",
    "\n",
    "# set i = 0 for the while loop\n",
    "i = 0\n",
    "# Loop through the datasets using the dataset ids\n",
    "while i < len(datasetid):\n",
    "    # The url is dependent upon the specific datasetid\n",
    "    url = \"https://nsidc.org/data/\" +datasetid[i]+ \"?qt-data_set_tabs=1#qt-data_set_tabs\"\n",
    "    # Pull the webpage using requests\n",
    "    r = requests.get(url)\n",
    "    # Create a BeautifulSoup object to hold the content of the url\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    # Create an object to hold the text data found to match the a tag starting with http://dx.doi\n",
    "    x_1 = [a.findAll(text=True) for a in (soup.find_all('a', href=re.compile('^http://dx.doi')))]\n",
    "    # Loop through x_1 and add the contents to the DOI list, if no contents add empty string\n",
    "    if len(x_1) == 0:\n",
    "        DOI.append(\"\")\n",
    "    else:\n",
    "        DOI.append(x_1)\n",
    "        \n",
    "    # Create an object that holds the data found to match html div class 'views-field views-field_nothing'\n",
    "    x_2 = soup.find_all(\"div\", {\"class\": \"views-field views-field-nothing\"})\n",
    "    # if nothing is found append an empty string to citation_date\n",
    "    if len(x_2) == 0:\n",
    "        citation_date.append(\"\")\n",
    "    # if something is found\n",
    "    else:\n",
    "        # Loop through the items in x_2 looking for span class \"date-display-single\" specifically in index 1 of contents\n",
    "        for item in x_2:\n",
    "            x_2_a = item.contents[1].find_all(\"span\", {\"class\": \"date-display-single\"})\n",
    "            # Pull out text from x_2_a\n",
    "            x_2_a = [a.findAll(text=True) for a in x_2_a]\n",
    "            # if x_2_a contents any text, append it to citation_date\n",
    "            if len(x_2_a) != 0:\n",
    "                citation_date.append(x_2_a)\n",
    "    # Update i            \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add lists from above code chunk to new columns in nsidc_df\n",
    "nsidc_df_rawscrape['scrape_date'] = scrape_date\n",
    "nsidc_df_rawscrape['scrape_time'] = scrape_time\n",
    "nsidc_df_rawscrape['data_format_original'] = data_format\n",
    "nsidc_df_rawscrape['contributors_original'] = contributors\n",
    "nsidc_df_rawscrape['spatial_coverage_original'] = spatial_coverage\n",
    "nsidc_df_rawscrape['spatial_resolution_original'] = spatial_resolution\n",
    "nsidc_df_rawscrape['temporal_coverage_original'] = temporal_coverage\n",
    "nsidc_df_rawscrape['temporal_resolution_original'] = temporal_resolution\n",
    "nsidc_df_rawscrape['parameters_original'] = parameters\n",
    "nsidc_df_rawscrape['platforms_original'] = platforms\n",
    "nsidc_df_rawscrape['sensors_original'] = sensors\n",
    "nsidc_df_rawscrape['version_original'] = version\n",
    "nsidc_df_rawscrape['doi_address'] = DOI\n",
    "nsidc_df_rawscrape['citation_date'] = citation_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>unique_users_ip</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>data_format_original</th>\n",
       "      <th>contributors_original</th>\n",
       "      <th>spatial_coverage_original</th>\n",
       "      <th>spatial_resolution_original</th>\n",
       "      <th>temporal_coverage_original</th>\n",
       "      <th>temporal_resolution_original</th>\n",
       "      <th>parameters_original</th>\n",
       "      <th>platforms_original</th>\n",
       "      <th>sensors_original</th>\n",
       "      <th>version_original</th>\n",
       "      <th>doi_address</th>\n",
       "      <th>citation_date</th>\n",
       "      <th>version_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g02186</td>\n",
       "      <td>24447</td>\n",
       "      <td>2016-05-18</td>\n",
       "      <td>10:45:22AM</td>\n",
       "      <td>\\nPNG\\nESRI Shapefile\\nNetCDF\\nMicrosoft Excel\\nKeyhole Markup Language (.kml)\\nASCII Text (.txt)\\nGeoTIFF\\n</td>\n",
       "      <td>\\n            Florence Fetterer, Pablo Clemente-Colón, Matthew Savoie, Sean Helfrich</td>\n",
       "      <td>\\nN: 90, S: 0, E: 180, W: -180\\n\\n</td>\n",
       "      <td>\\n4 km x 4 km\\n1 km x 1 km\\n</td>\n",
       "      <td>\\n1 October 2006\\n (updated daily)</td>\n",
       "      <td>\\n            1 day</td>\n",
       "      <td>\\nSea Ice &gt; Ice Edges\\nSea Ice &gt; Ice Extent\\nSea Ice &gt; Ice Growth/Melt\\n</td>\n",
       "      <td>\\n            ALOS, AQUA, DMSP, ENVISAT, ERS-2, GOES, MSG, NOAA POES, RADARSAT-2, SATELLITES</td>\n",
       "      <td>\\nAMSR-E, AMSU-A, AMSU-B, ASAR, AVHRR, GOES I-M IMAGER, MODIS, PALSAR, SAR, SEVIRI, SSM/I</td>\n",
       "      <td>\\n            V1</td>\n",
       "      <td>[[http://dx.doi.org/10.7265/N5GT5K3K]]</td>\n",
       "      <td>[[2010]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g02135</td>\n",
       "      <td>22119</td>\n",
       "      <td>2016-05-18</td>\n",
       "      <td>10:45:22AM</td>\n",
       "      <td>\\nPNG\\nASCII Text (.txt)\\nESRI Shapefile\\n</td>\n",
       "      <td>\\n            F. Fetterer, Kenneth Knowles, Walt Meier, Matthew Savoie</td>\n",
       "      <td>\\nN: -39.23, S: -90, E: 180, W: -180\\n\\nN: 90, S: 30.98, E: 180, W: -180\\n\\n</td>\n",
       "      <td>\\n25 km x 25 km\\n</td>\n",
       "      <td>\\n26 October 1978\\n (updated daily)</td>\n",
       "      <td>\\n            1 day</td>\n",
       "      <td>\\nSea Ice &gt; Ice Extent\\nSea Ice &gt; Ice Growth/Melt\\nSea Ice &gt; Sea Ice Concentration\\n</td>\n",
       "      <td>\\n            DMSP, DMSP 5D-3/F17, NIMBUS-7, SATELLITES</td>\n",
       "      <td>\\nSMMR, SSM/I, SSMIS</td>\n",
       "      <td>\\n            V1</td>\n",
       "      <td>[[http://dx.doi.org/10.7265/N5QJ7F7W]]</td>\n",
       "      <td>[[2002]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           dataset_id  unique_users_ip  \\\n",
       "0  g02186                                                        24447   \n",
       "1  g02135                                                        22119   \n",
       "\n",
       "  scrape_date scrape_time  \\\n",
       "0  2016-05-18  10:45:22AM   \n",
       "1  2016-05-18  10:45:22AM   \n",
       "\n",
       "                                                                                            data_format_original  \\\n",
       "0  \\nPNG\\nESRI Shapefile\\nNetCDF\\nMicrosoft Excel\\nKeyhole Markup Language (.kml)\\nASCII Text (.txt)\\nGeoTIFF\\n    \n",
       "1                                                                    \\nPNG\\nASCII Text (.txt)\\nESRI Shapefile\\n    \n",
       "\n",
       "                                                                            contributors_original  \\\n",
       "0  \\n            Florence Fetterer, Pablo Clemente-Colón, Matthew Savoie, Sean Helfrich             \n",
       "1                \\n            F. Fetterer, Kenneth Knowles, Walt Meier, Matthew Savoie             \n",
       "\n",
       "                                                       spatial_coverage_original  \\\n",
       "0                                            \\nN: 90, S: 0, E: 180, W: -180\\n\\n    \n",
       "1  \\nN: -39.23, S: -90, E: 180, W: -180\\n\\nN: 90, S: 30.98, E: 180, W: -180\\n\\n    \n",
       "\n",
       "     spatial_resolution_original  \\\n",
       "0  \\n4 km x 4 km\\n1 km x 1 km\\n    \n",
       "1             \\n25 km x 25 km\\n    \n",
       "\n",
       "                      temporal_coverage_original  \\\n",
       "0   \\n1 October 2006\\n (updated daily)             \n",
       "1  \\n26 October 1978\\n (updated daily)             \n",
       "\n",
       "    temporal_resolution_original  \\\n",
       "0  \\n            1 day             \n",
       "1  \\n            1 day             \n",
       "\n",
       "                                                                     parameters_original  \\\n",
       "0              \\nSea Ice > Ice Edges\\nSea Ice > Ice Extent\\nSea Ice > Ice Growth/Melt\\n    \n",
       "1  \\nSea Ice > Ice Extent\\nSea Ice > Ice Growth/Melt\\nSea Ice > Sea Ice Concentration\\n    \n",
       "\n",
       "                                                                                       platforms_original  \\\n",
       "0  \\n            ALOS, AQUA, DMSP, ENVISAT, ERS-2, GOES, MSG, NOAA POES, RADARSAT-2, SATELLITES             \n",
       "1                                       \\n            DMSP, DMSP 5D-3/F17, NIMBUS-7, SATELLITES             \n",
       "\n",
       "                                                                             sensors_original  \\\n",
       "0  \\nAMSR-E, AMSU-A, AMSU-B, ASAR, AVHRR, GOES I-M IMAGER, MODIS, PALSAR, SAR, SEVIRI, SSM/I    \n",
       "1                                                                       \\nSMMR, SSM/I, SSMIS    \n",
       "\n",
       "             version_original                             doi_address  \\\n",
       "0  \\n            V1            [[http://dx.doi.org/10.7265/N5GT5K3K]]   \n",
       "1  \\n            V1            [[http://dx.doi.org/10.7265/N5QJ7F7W]]   \n",
       "\n",
       "  citation_date version_clean  \n",
       "0      [[2010]]   1            \n",
       "1      [[2002]]   1            "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty list\n",
    "version = []\n",
    "# Set i = 0 for while loop\n",
    "i=0\n",
    "# Loop through rows in nsidc_df\n",
    "while i < len(nsidc_df_rawscrape):\n",
    "    try:\n",
    "        # Remove \\n occurrences\n",
    "        thing = re.sub(\"\\\\\\n\", \"\", nsidc_df_rawscrape.loc[i, 'version_original'])\n",
    "        # Create a regex to compile everything after V\n",
    "        regexp = re.compile(\"V(.*)$\", re.I)\n",
    "        str1 = regexp.search(thing).group(1)\n",
    "        if str1 == \"None\":\n",
    "            version.append(\"\")\n",
    "        else:\n",
    "            version.append(str1)\n",
    "        i+=1\n",
    "    # If there was an error in the try code append the empty string, update i, iterate\n",
    "    except:\n",
    "        version.append(\"\")\n",
    "        i+=1\n",
    "        pass\n",
    "\n",
    "# Add column to dataframe with new data\n",
    "nsidc_df_rawscrape['version_clean'] = version\n",
    "# Check results\n",
    "nsidc_df_rawscrape.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        g02135\n",
      "2        g00472\n",
      "3    nsidc-0081\n",
      "4    nsidc-0051\n",
      "Name: dataset_id, dtype: object\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: version_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create a list of NSIDC Version #s to scrape course data from (used in url)\n",
    "nsidc_df_rawscrape['version_clean'] = nsidc_df_rawscrape['version_clean'].astype('str')\n",
    "version = nsidc_df_rawscrape['version_clean'].str.strip()\n",
    "print datasetid[1:5]\n",
    "print version[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BeautifulSoup code and requests code adapted from https://www.youtube.com/watch?v=f2h41uEi0xU \n",
    "# and https://www.youtube.com/watch?v=3xQTJi2tqgk\n",
    "\n",
    "# Create empty lists to append data\n",
    "location_original = []\n",
    "keyword_original = []\n",
    "date_creation_original = []\n",
    "last_updated_original = []\n",
    "title_original = []\n",
    "\n",
    "\n",
    "# set i = 0 for the while loop\n",
    "i = 0\n",
    "# Loop through the datasets using the dataset ids\n",
    "while i < len(datasetid):\n",
    "    # The url is dependent upon the specific datasetid and version number\n",
    "    url = \"https://nsidc.org/data/\" +datasetid[i]+ \"/versions/\" +version[i]+ \"/metadata\"\n",
    "    # Pull the webpage using requests\n",
    "    r = requests.get(url)\n",
    "    # Create a BeautifulSoup object to hold the content of the url\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    # Create an object to hold the data found to match the html h2 tag\n",
    "    title = soup.findAll('h2')\n",
    "    if len(title) == 0:\n",
    "        title_original.append(\"\")\n",
    "    else:\n",
    "        # The first element is the title of the dataset - append to title list\n",
    "        title_original.append(title[0].text)\n",
    "    # Create an object to hold the data found to match the html div class for location\n",
    "    x_1 = soup.find_all(\"div\", {\"class\": \"field field-name-field-dataset-location field-type-text field-label-above\"})\n",
    "    # Loop through each item and add the contents to the location list, if no contents add empty string\n",
    "    if len(x_1) == 0:\n",
    "        location_original.append(\"\")\n",
    "    else:\n",
    "        for item in x_1:\n",
    "            location_original.append(item.text)\n",
    "    # Create an object to hold the data found to match the html div class for keyword\n",
    "    x_2 = soup.find_all(\"div\", {\"class\": \"field field-name-field-dataset-keyword field-type-text field-label-above\"})\n",
    "    # Loop through each item, if no contents add empty string\n",
    "    if len(x_2) == 0:\n",
    "        keyword_original.append(\"\")\n",
    "    else:\n",
    "        # Loop through the current contenst of x_2 looking for div class \"field item\"\n",
    "        keywords = x_2[0].findAll(\"div\", {\"class\": \"field-item\"})\n",
    "        # Create empty list\n",
    "        thingy = []\n",
    "        # For each object in keywords, append the text\n",
    "        for word in range(len(keywords)):\n",
    "            thingy.append(keywords[word].text)\n",
    "        # Append the contents of thingy to the keyword list\n",
    "        keyword_original.append(thingy)\n",
    "    # Create an object to hold the data found to match the html div class for datetime\n",
    "    x_3 = soup.find_all(\"div\", {\"class\": \"field field-name-field-dataset-dc-date field-type-datetime field-label-inline clearfix\"})\n",
    "    # Loop through each item, if no contents add empty string\n",
    "    if len(x_3) == 0:\n",
    "        date_creation_original.append(\"\")\n",
    "    else:\n",
    "        # Loop through the items in x_3 looking for span class \"date-display-single\" specifically in index 1 of contents\n",
    "        for item in x_3:\n",
    "            x_3_a = item.contents[1].find_all(\"span\", {\"class\": \"date-display-single\"})\n",
    "            # Pull out text from x_3_a\n",
    "            x_3_a = [a.findAll(text=True) for a in x_3_a]\n",
    "            # if x_3_a contains any text, append it to date_creation_original\n",
    "            if len(x_3_a) != 0:\n",
    "                date_creation_original.append(x_3_a)\n",
    "    # Create an object to hold the data found to match the html div class for datetime\n",
    "    x_4 = soup.find_all(\"div\", {\"class\": \"field field-name-field-dataset-last-updated field-type-datetime field-label-inline clearfix\"})\n",
    "    # Loop through each item, if no contents add empty string\n",
    "    if len(x_4) == 0:\n",
    "        last_updated_original.append(\"\")\n",
    "    else:\n",
    "        # Loop through the items in x_4 looking for span class \"date-display-single\" specifically in index 1 of contents\n",
    "        for item in x_4:\n",
    "            x_4_a = item.contents[1].find_all(\"span\", {\"class\": \"date-display-single\"})\n",
    "            # Pull out text from x_4_a\n",
    "            x_4_a = [a.findAll(text=True) for a in x_4_a]\n",
    "            # if x_4_a contents any text, append it to last_updated_original\n",
    "            if len(x_4_a) != 0:\n",
    "                last_updated_original.append(x_4_a)\n",
    "    # Increase i by 1 and run through loop\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add lists from above code chunk to new columns in nsidc_df\n",
    "nsidc_df_rawscrape['location_original'] = location_original\n",
    "nsidc_df_rawscrape['keyword_original'] = keyword_original\n",
    "nsidc_df_rawscrape['date_creation_original'] = date_creation_original\n",
    "nsidc_df_rawscrape['last_updated_original'] = last_updated_original\n",
    "nsidc_df_rawscrape['title_original'] = title_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Capture the current date and time in object nowtime\n",
    "nowtime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "# Save dataframe to csv\n",
    "nsidc_df_rawscrape.to_csv('nsidc_df_rawscrape_' + nowtime + '.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
